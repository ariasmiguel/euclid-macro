"""
Enhanced debug script for S&P 500 fetcher with VPN-like approaches
"""

import logging
import requests
import time
import random
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from src.fetchers.fetch_sp500 import SP500Fetcher
from src.utils.web_scraping_utils import WebScrapingUtils

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def get_random_user_agent():
    """Get a random realistic user agent"""
    user_agents = [
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    return random.choice(user_agents)

def test_session_based_download():
    """Test session-based download with better session management"""
    print("ğŸ” Testing session-based download with enhanced session management...")
    
    url = "https://www.spglobal.com/spdji/en/documents/additional-material/sp-500-eps-est.xlsx"
    
    # Create a session to maintain cookies
    session = requests.Session()
    
    # Set realistic headers
    session.headers.update({
        'User-Agent': get_random_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0'
    })
    
    try:
        # First, visit the referer page to establish session
        referer_url = "https://www.spglobal.com/spdji/en/indices/equity/sp-500/"
        print(f"ğŸŒ Visiting referer page: {referer_url}")
        
        referer_response = session.get(referer_url, timeout=30)
        print(f"ğŸ“Š Referer response status: {referer_response.status_code}")
        
        if referer_response.status_code == 200:
            print("âœ… Referer page accessed successfully")
            time.sleep(2)  # Small delay to mimic human behavior
        else:
            print(f"âš ï¸ Referer page returned status: {referer_response.status_code}")
        
        # Now try to download the file
        print(f"ğŸ“¥ Attempting download from: {url}")
        
        # Update headers for file download
        session.headers.update({
            'Accept': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet,application/vnd.ms-excel,*/*',
            'Referer': referer_url,
        })
        
        response = session.get(url, timeout=30, stream=True)
        
        print(f"ğŸ“Š Download response status: {response.status_code}")
        print(f"ğŸ“Š Response headers: {dict(response.headers)}")
        
        if response.status_code == 200:
            # Save the file
            download_dir = Path("data/raw/sp500")
            download_dir.mkdir(parents=True, exist_ok=True)
            
            file_path = download_dir / "sp-500-eps-est.xlsx"
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"âœ… File downloaded successfully: {file_path}")
            print(f"ğŸ“Š File size: {file_path.stat().st_size} bytes")
            return str(file_path)
        else:
            print(f"âŒ Download failed with status code: {response.status_code}")
            print(f"ğŸ“„ Response content: {response.text[:500]}...")
            return None
            
    except Exception as e:
        print(f"âŒ Session-based download failed: {str(e)}")
        return None

def test_enhanced_selenium_with_stealth():
    """Test Selenium with enhanced stealth and VPN-like approaches"""
    print("ğŸ” Testing enhanced Selenium with stealth settings...")
    
    download_dir = Path("data/raw/sp500")
    download_dir.mkdir(parents=True, exist_ok=True)
    
    # Clear any existing files
    for file in download_dir.glob("*.xlsx"):
        file.unlink()
    
    scraper = None
    try:
        # Use the existing WebScrapingUtils with stealth
        scraper = WebScrapingUtils.create_driver_with_defaults(
            download_dir=str(download_dir),
            headless=True,
            apply_stealth=True
        )
        
        driver = scraper.driver
        
        # Test referer page with longer wait
        referer_url = "https://www.spglobal.com/spdji/en/indices/equity/sp-500/"
        print(f"ğŸŒ Visiting referer page: {referer_url}")
        
        # Use the utility method for navigation with delay
        scraper.navigate_with_delay(referer_url, min_delay=3.0, max_delay=6.0)
        
        print(f"ğŸ“„ Page title: {driver.title}")
        print(f"ğŸ“„ Current URL: {driver.current_url}")
        
        # Check if we got an error page
        if "error" in driver.title.lower():
            print("ğŸš« Error page detected on referer")
            print(f"ğŸ“„ Page source preview: {driver.page_source[:500]}...")
            return None
        
        # Try to find any download links on the page
        try:
            links = driver.find_elements(By.TAG_NAME, "a")
            excel_links = [link for link in links if link.get_attribute("href") and ".xlsx" in link.get_attribute("href")]
            print(f"ğŸ”— Found {len(excel_links)} Excel links on the page")
            
            for i, link in enumerate(excel_links[:3]):  # Show first 3
                href = link.get_attribute("href")
                text = link.text.strip()
                print(f"   {i+1}. {text} -> {href}")
        except Exception as e:
            print(f"âš ï¸ Could not search for links: {e}")
        
        # Now try the direct download URL
        download_url = "https://www.spglobal.com/spdji/en/documents/additional-material/sp-500-eps-est.xlsx"
        print(f"ğŸ“¥ Attempting to download from: {download_url}")
        
        # Navigate to download URL with delay
        scraper.navigate_with_delay(download_url, min_delay=5.0, max_delay=8.0)
        
        print(f"ğŸ“„ Final URL: {driver.current_url}")
        print(f"ğŸ“„ Page title: {driver.title}")
        
        # Check for downloaded files
        excel_files = list(download_dir.glob("*.xlsx"))
        print(f"ğŸ“ Files in download directory: {[f.name for f in excel_files]}")
        
        if excel_files:
            latest_file = max(excel_files, key=lambda x: x.stat().st_mtime)
            print(f"âœ… File downloaded: {latest_file}")
            print(f"ğŸ“Š File size: {latest_file.stat().st_size} bytes")
            return str(latest_file)
        else:
            print("âŒ No files downloaded")
            
            # Check if we got redirected to an error page
            if "error" in driver.current_url.lower() or "access denied" in driver.page_source.lower():
                print("ğŸš« Access denied or error page detected")
                print(f"ğŸ“„ Page source preview: {driver.page_source[:500]}...")
            
            return None
            
    except Exception as e:
        print(f"âŒ Enhanced Selenium download failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return None
        
    finally:
        if scraper:
            scraper.cleanup_driver()

def test_alternative_urls():
    """Test alternative URLs that might work"""
    print("ğŸ” Testing alternative URLs...")
    
    alternative_urls = [
        "https://www.spglobal.com/spdji/en/documents/additional-material/sp-500-eps-est.xlsx",
        "https://www.spglobal.com/spdji/en/indices/equity/sp-500/#overview",
        "https://www.spglobal.com/spdji/en/indices/equity/sp-500/",
    ]
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': get_random_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
    })
    
    for url in alternative_urls:
        try:
            print(f"ğŸ”— Testing URL: {url}")
            response = session.get(url, timeout=10)
            print(f"   Status: {response.status_code}")
            
            if response.status_code == 200:
                print(f"   âœ… URL accessible")
                if "xlsx" in url:
                    return url
            else:
                print(f"   âŒ URL not accessible")
                
        except Exception as e:
            print(f"   âŒ Error: {e}")
    
    return None

def test_sp500_fetcher():
    """Test the S&P 500 fetcher with detailed debugging"""
    print("ğŸ§ª Testing S&P 500 fetcher with detailed debugging...")
    
    try:
        # Create fetcher with explicit download directory
        fetcher = SP500Fetcher(download_dir="data/raw/sp500")
        
        # Test the fetcher
        data = fetcher.fetch_batch()
        
        if not data.empty:
            print(f"âœ… S&P 500 fetcher successful!")
            print(f"ğŸ“Š Data shape: {data.shape}")
            print(f"ğŸ“… Date range: {data['date'].min()} to {data['date'].max()}")
            print(f"ğŸ“ˆ Metrics: {data['metric'].unique().tolist()}")
            print(f"ğŸ”¢ Sample data:")
            print(data.head())
        else:
            print("âŒ S&P 500 fetcher returned empty data")
            
    except Exception as e:
        print(f"âŒ S&P 500 fetcher failed: {str(e)}")
        import traceback
        traceback.print_exc()

def main():
    """Run all tests"""
    print("ğŸš€ Starting enhanced S&P 500 fetcher debugging...")
    print("=" * 60)
    
    # Test 1: Session-based download
    print("\n1ï¸âƒ£ Testing session-based download...")
    session_result = test_session_based_download()
    
    # Test 2: Enhanced Selenium with stealth
    print("\n2ï¸âƒ£ Testing enhanced Selenium with stealth...")
    selenium_result = test_enhanced_selenium_with_stealth()
    
    # Test 3: Alternative URLs
    print("\n3ï¸âƒ£ Testing alternative URLs...")
    alt_url = test_alternative_urls()
    
    # Test 4: Original fetcher
    print("\n4ï¸âƒ£ Testing original fetcher...")
    test_sp500_fetcher()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ Summary:")
    print(f"   Session-based download: {'âœ…' if session_result else 'âŒ'}")
    print(f"   Enhanced Selenium: {'âœ…' if selenium_result else 'âŒ'}")
    print(f"   Alternative URL found: {'âœ…' if alt_url else 'âŒ'}")
    
    if session_result or selenium_result:
        print("ğŸ‰ At least one download method worked!")
    else:
        print("ğŸ’¡ Next steps to try:")
        print("   - Wait a few hours and try again (rate limiting)")
        print("   - Use a VPN to change IP address")
        print("   - Try different user agents")
        print("   - Check if the website requires registration/login")
        print("   - Look for alternative data sources")

if __name__ == "__main__":
    main() 