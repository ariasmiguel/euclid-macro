"""
Main Data Collection Pipeline Entry Point

This script provides a simple entry point for running the complete
data collection pipeline using DuckDB and bronze layer exports
with incremental loading support.
"""

import argparse
import logging
import sys
from datetime import datetime

# Import the main pipeline class
from src_pipeline.pipelines.data_collection import DataCollectionPipeline
from src_pipeline.core.date_utils import DateUtils

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def main():
    """
    Main function that orchestrates the entire data collection pipeline.
    """
    parser = argparse.ArgumentParser(
        description='Bristol Gate Data Collection Pipeline with Incremental Loading',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python run_data_collection.py                                 # Incremental loading (default)
    python run_data_collection.py --full-refresh                  # Full refresh (reload all data)
    python run_data_collection.py --incremental                   # Explicit incremental mode
    python run_data_collection.py --sources yahoo,fred,baker      # Only collect from specific sources
    python run_data_collection.py --sources yahoo --full-refresh  # Full refresh for Yahoo only

Available Sources:
    ‚Ä¢ yahoo     - Yahoo Finance stock/ETF data
    ‚Ä¢ fred      - Federal Reserve Economic Data (FRED)
    ‚Ä¢ eia       - Energy Information Administration
    ‚Ä¢ baker     - Baker Hughes rig count data
    ‚Ä¢ finra     - FINRA margin statistics
    ‚Ä¢ sp500     - S&P 500 earnings/estimates (Silverblatt)
    ‚Ä¢ usda      - USDA agricultural data

Features:
    ‚Ä¢ Incremental loading: Only fetches data newer than existing data
    ‚Ä¢ Source filtering: Test specific sources during development
    ‚Ä¢ Respects API rate limits (especially important for FRED)
    ‚Ä¢ Append-only bronze layer for complete audit trail
    ‚Ä¢ Fast pipeline execution with optimal DuckDB storage
        """
    )
    
    parser.add_argument('--incremental', action='store_true', default=True,
                       help='Use incremental loading (default)')
    parser.add_argument('--full-refresh', action='store_true',
                       help='Perform full refresh (reload all data)')
    parser.add_argument('--sources', type=str,
                       help='Comma-separated list of sources to collect (e.g., yahoo,fred,baker). Leave empty for all sources.')
    
    args = parser.parse_args()
    
    # Parse sources list
    allowed_sources = None
    if args.sources:
        allowed_sources = [source.strip() for source in args.sources.split(',')]
        logger.info(f"üéØ Source filtering: {', '.join(allowed_sources)}")
    
    # Determine incremental mode
    incremental_mode = not args.full_refresh
    
    logger.info("üöÄ Bristol Gate Data Collection Pipeline")
    logger.info(f"‚è∞ Started at: {DateUtils.format_current_datetime()}")
    logger.info(f"üìã Mode: {'INCREMENTAL' if incremental_mode else 'FULL REFRESH'}")
    
    try:
        # Initialize and run the data collection pipeline
        pipeline = DataCollectionPipeline(incremental=incremental_mode, allowed_sources=allowed_sources)
        success = pipeline.run_full_pipeline()
        
        if not success:
            logger.error("‚ùå Data collection pipeline failed")
            sys.exit(1)
        else:
            logger.info("üéâ Data collection pipeline completed successfully!")
            logger.info("üìä Data is now available in:")
            logger.info("   ‚Ä¢ DuckDB staging tables (bristol_gate.duckdb)")
            logger.info("   ‚Ä¢ Bronze layer parquet files (data/bronze/)")
            
            if incremental_mode:
                logger.info("üí° Tip: Use --full-refresh to reload all data if needed")
            
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è  Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Fatal error in data collection pipeline: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
